# Voice-Enabled RAG Tutor

A selfâ€‘hosted, voiceâ€‘enabled Retrievalâ€‘Augmentedâ€‘Generation (RAG) tutor that can serve as an expert in *any* domainâ€”from computer vision to NLP, finance, healthcare, and beyond. Speak your question into the mic; the system transcribes it, retrieves relevant document chunks, generates a grounded answer with a local LLM, and returns both text and TTS audioâ€”complete with avatar animations.

---

## ğŸ“‚ Project Structure

```plaintext
.
â”œâ”€â”€ Avatar/                   # waiting & speaking avatar animations (MP4)
â”‚   â”œâ”€â”€ avatar waiting.mp4
â”‚   â””â”€â”€ avatar talking.mp4
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml        # optional configuration flags
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # put your domain PDFs & PPTX files here
â”‚   â””â”€â”€ chunks/              # autoâ€‘generated JSON document chunks
â”œâ”€â”€ db/
â”‚   â””â”€â”€ chroma_index/        # persisted Chroma vectorstore
â”œâ”€â”€ offline/
â”‚   â”œâ”€â”€ loaders.py           # load PDFs + PPTX â†’ LangChain Documents
â”‚   â”œâ”€â”€ splitter.py          # group pages & split into text chunks
â”‚   â””â”€â”€ indexer.py           # embed & persist chunks into Chroma
â”œâ”€â”€ online/
â”‚   â”œâ”€â”€ stt/
â”‚   â”‚   â”œâ”€â”€ whisper_stt.py   # fasterâ€‘whisper wrapper (English only)
â”‚   â”‚   â””â”€â”€ record_test.py   # CLI for mic testing & transcription
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â””â”€â”€ retriever.py     # load Chroma & similarity search
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ inference.py     # build prompt, call OllamaLLM, format citations
â”‚   â”œâ”€â”€ tts/
â”‚   â”‚   â””â”€â”€ tts_service.py   # synthesize answer to WAV via TTS engine
â”‚   â”œâ”€â”€ temp/                # working audio files (in/out)
â”‚   â””â”€â”€ server.py            # FastAPI app (endpoints `/ask/` & `/chat/`)
â”œâ”€â”€ index.html               # browser UI (record, display, playback)
â”œâ”€â”€ README.md                # this file
â”œâ”€â”€ requirements.txt         # pinned dependencies
â””â”€â”€ test_output.wav          # example recording
```

---

## âš™ï¸ Installation

1. **Clone & enter**

   ```bash
   git clone <repo_url> && cd voiceâ€‘chatbot
   ```

2. **Python env & install**

   ```bash
   python -m venv .venv
   source .venv/bin/activate      # Linux/macOS
   .venv\Scripts\activate.bat    # Windows
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

> No OCR or Poppler/Tesseract neededâ€”PDFs load directly with `PyPDFLoader`, PPTX via `python-pptx`.

---

## ğŸ› ï¸ Offline Indexing

1. **Split** your domain materials (PDF + PPTX â†’ JSON chunks):

   ```bash
   python offline/splitter.py
   ```

2. **Embed & persist** chunks into Chroma:

   ```bash
   python offline/indexer.py
   ```

---

## ğŸš€ Running the API

```bash
uvicorn online.server:app --reload --port 8000
```

* **POST** `/ask/` (audio upload) â†’ returns JSON:

  ```jsonc
  {
    "transcript": "user question text",
    "answer":     "LLMâ€™s grounded answer",
    "citation":   "- file.pdf (page X)\n- other.pdf (page Y)",
    "audio_url":  "/audio/<uid>_out.wav",
    "avatar_waiting":  "/static/avatar waiting.mp4",
    "avatar_speaking": "/static/avatar talking.mp4"
  }
  ```

* **POST** `/chat/` (form text) â†’ pure-text chat without recording.

---

## ğŸ¤ Web UI

Browse to [http://127.0.0.1:8000](http://127.0.0.1:8000):

1. **Record** your question via mic.
2. **Stop** â†’ see **Transcript**, **Answer**, and hear audio playback + avatar animation.

---

## ğŸ”§ Customization

* **Change LLM**: edit `online/llm/inference.py` (e.g. use `OllamaLLM` or other).
* **Chunking**: adjust `pages_per_chunk`, `chunk_size`, `chunk_overlap` in `offline/splitter.py`.
* **Retrieval**: tweak `top_k` or threshold in `online/retrieval/retriever.py`.
* **STT**: swap `model_size_or_path` or `device` in `online/stt/whisper_stt.py`.
* **TTS**: point `synthesize(...)` to your favorite TTS in `online/tts/tts_service.py`.

---

## ğŸ“ Troubleshooting

* **Empty transcription?**

  * Check mic permissions & audio format.
  * Use `python online/stt/record_test.py` to record + transcribe.

* **Missing content in answers?**

  * Ensure `data/raw/` files contain selectable text (not scanned images).
  * Tweak loader in `offline/loaders.py` if needed.

* **Chroma issues?**

  * Delete `db/chroma_index/` and re-run `offline/indexer.py`.

Happy teaching & learning across *any* domain!
â€” Your Voiceâ€‘Enabled RAG Tutor
