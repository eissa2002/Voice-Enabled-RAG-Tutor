# Voice-Enabled RAG Tutor

A selfâ€‘hosted, voiceâ€‘enabled Retrievalâ€‘Augmentedâ€‘Generation (RAG) tutor that can serve as an expert in *any* domainâ€”from computer vision to NLP, finance, healthcare, and beyond. Speak your question into the mic; the system transcribes it, retrieves relevant document chunks, generates a grounded answer with a local LLM, and returns both text and TTS audioâ€”complete with avatar animations.

---

## ğŸ“‚ Project Structure

```plaintext
.
â”œâ”€â”€ Avatar/                   # waiting & speaking avatar animations (MP4)
â”‚   â”œâ”€â”€ avatar waiting.mp4
â”‚   â””â”€â”€ avatar talking.mp4
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml         # optional configuration flags
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # put your domain PDFs & PPTX files here
â”‚   â””â”€â”€ chunks/               # autoâ€‘generated JSON document chunks
â”œâ”€â”€ db/
â”‚   â””â”€â”€ chroma_index/         # persisted Chroma vectorstore
â”œâ”€â”€ offline/                  # four-step offline indexing pipeline
â”‚   â”œâ”€â”€ loaders.py            # (1) load PDFs + PPTX â†’ Documents
â”‚   â”œâ”€â”€ splitter.py           # (2) group pages & split into text chunks
â”‚   â”œâ”€â”€ embedder.py           # (3) sanitize & prepare chunk metadata
â”‚   â””â”€â”€ indexer.py            # (4) embed & persist chunks into Chroma
â”œâ”€â”€ online/
â”‚   â”œâ”€â”€ stt/
â”‚   â”‚   â”œâ”€â”€ whisper_stt.py    # fasterâ€‘whisper wrapper (English-only)
â”‚   â”‚   â””â”€â”€ record_test.py    # CLI for mic testing & transcription
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â””â”€â”€ retriever.py      # load Chroma & similarity search
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ inference.py      # build prompt, call LLM, format citations
â”‚   â”œâ”€â”€ tts/
â”‚   â”‚   â””â”€â”€ tts_service.py    # synthesize answer to WAV via TTS engine
â”‚   â”œâ”€â”€ temp/                 # working audio files (in/out)
â”‚   â””â”€â”€ server.py             # FastAPI app (endpoints `/ask/` & `/chat/`)
â”œâ”€â”€ index.html                # browser UI (record, display, playback)
â”œâ”€â”€ README.md                 # this file
â”œâ”€â”€ requirements.txt          # pinned dependencies
â””â”€â”€ test_output.wav           # example recording
```

---

## ğŸ› ï¸ Offline Indexing (4 Steps)

1. **Load documents** (`offline/loaders.py`)
   Reads PDF & PPTX files into LangChain `Document` objects with metadata.
2. **Split into chunks** (`offline/splitter.py`)
   Groups pages, then splits text into manageable chunks (size & overlap tunable).
3. **Embed metadata** (`offline/embedder.py`)
   Sanitizes metadata for Chroma; prepares chunk payloads.
4. **Index in Chroma** (`offline/indexer.py`)
   Embeds chunks with HuggingFace model, persists vectorstore in `db/chroma_index`.

---

## ğŸš€ Pipeline Flowchart

```mermaid
flowchart TB
    A[User Records Question] --> B[STT: WhisperModel]
    B --> C[Text Transcript]
    C --> D[Retrieval: Chroma Similarity Search]
    D --> E[LLM: Prompt & Generate]
    E --> F[Text Answer]
    F --> G[TTS: Synthesize Audio]
    G --> H[Response: Text & Voice]
```

> **Note:** Responses include both the **text** answer (displayed on UI or JSON) and **voice** (audio playback).

---

## âš™ï¸ Installation & Setup

1. **Clone & enter**

   ```bash
   git clone <repo_url> && cd voiceâ€‘chatbot
   ```

2. **Python env & install**

   ```bash
   python -m venv .venv
   source .venv/bin/activate    # Linux/macOS
   .venv\Scripts\activate.bat  # Windows
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

> No OCR/Poppler neededâ€”PDFs load with `PyPDFLoader`, PPTX via `python-pptx`.

---

## ğŸš€ Running the API

```bash
uvicorn online.server:app --reload --port 8000
```

* **POST** `/ask/` (audio upload) â†’ returns JSON with `transcript`, `answer`, `citation`, `audio_url` and avatar URLs.
* **POST** `/chat/` (form text) â†’ returns pure-text + optional audio chat.

---

## ğŸ¤ Web UI

Visit `http://127.0.0.1:8000`:

1. **Record** your question with mic.
2. **Stop** â†’ see **Transcript**, **Answer**, and hear audio with avatar animation.

---

## ğŸ”§ Customization

* **LLM**: edit `online/llm/inference.py` to swap or tune the model.
* **Chunking**: adjust parameters in `offline/splitter.py` (size, overlap).
* **Retrieval**: tweak `top_k` or score threshold in `online/retrieval/retriever.py`.
* **STT**: choose model size/device in `online/stt/whisper_stt.py`.
* **TTS**: configure `online/tts/tts_service.py` for your preferred engine.

---

## ğŸ“ Troubleshooting

* **No transcript?**

  * Check mic permissions & format.
  * Use `python online/stt/record_test.py` to record + transcribe.

* **Incomplete answers?**

  * Ensure `data/raw/` contains selectable text (not images).
  * Adjust loader in `offline/loaders.py` or fallback settings.

* **Chroma errors?**

  * Remove `db/chroma_index/` and re-run indexing.

Happy teaching & learning across *any* domain! ğŸ“
